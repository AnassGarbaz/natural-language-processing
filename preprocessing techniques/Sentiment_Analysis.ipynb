{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sentiment_Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Aw1iGm0Tyhni"
      },
      "source": [
        "#SentimentAnalyzer : "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JwwCrauKysLb"
      },
      "source": [
        "* ## 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Akcxr5bcOck"
      },
      "source": [
        "----\n",
        "\n",
        "Import the “Subjectivity” corpus using from nltk.corpus import subjectivity\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y1oq6jNfyVJF",
        "outputId": "d2a9e1ab-989a-4547-f374-fe435d7a9eca"
      },
      "source": [
        "from nltk.corpus import subjectivity\n",
        "import nltk\n",
        "nltk.download('subjectivity')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package subjectivity to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/subjectivity.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ii0gg6Kz2E8V"
      },
      "source": [
        "* ## 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F-9FxglicYUW"
      },
      "source": [
        "---\n",
        "The dataset contains two categories of instances: ’subj’ and ’obj’. let's create a training set and a test set containing, respectively, 320 and 80 instances of each category using subjectivity.sents (categories = ...).\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyBO1NwUyrdq"
      },
      "source": [
        "train_instances = 320\n",
        "test_instances = 80\n",
        "n_inst = 400\n",
        "# Each document is represented by a tuple (sentence, label).\n",
        "# The sentence is tokenized, so it is represented by a list of strings:\n",
        "subj_docs = [(sent, 'subj') for sent in subjectivity.sents(categories='subj')[:n_inst]]\n",
        "obj_docs = [(sent, 'obj') for sent in subjectivity.sents(categories='obj')[:n_inst]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieBfcxw62nPu"
      },
      "source": [
        "sliceTrain = int(train_instances/2)\n",
        "sliceTest = int(test_instances/2)\n",
        "train_subj_docs = subj_docs[:sliceTrain]\n",
        "test_subj_docs = subj_docs[sliceTrain:sliceTrain+sliceTest]\n",
        "train_obj_docs = obj_docs[:sliceTrain]\n",
        "test_obj_docs = obj_docs[sliceTrain:sliceTrain+sliceTest]\n",
        "training_docs = train_subj_docs+train_obj_docs\n",
        "testing_docs = test_subj_docs+test_obj_docs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WTIeO-r86lsL",
        "outputId": "452b9c93-8d5d-4fd8-c4dd-f1a5d547d385"
      },
      "source": [
        "len(testing_docs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "80"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KdQ9NLcC8sMw",
        "outputId": "00e9e522-a500-40a3-da8c-56ca4a817af6"
      },
      "source": [
        "len(training_docs)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "320"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Osns2xgo6ZKc"
      },
      "source": [
        "* ## 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1Uax-Ufc4RC"
      },
      "source": [
        "---\n",
        "\n",
        "Use the mark_negation function to highlight negations in the data.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zV-vXqSl6TDV",
        "outputId": "d25c2897-9fe4-45cf-b93f-1231e38fddf4"
      },
      "source": [
        "from nltk.sentiment.util import *"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/nltk/twitter/__init__.py:20: UserWarning: The twython library has not been installed. Some functionality from the twitter package will not be available.\n",
            "  warnings.warn(\"The twython library has not been installed. \"\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3Mbjb2gA_Fd4"
      },
      "source": [
        "neg = [mark_negation(training_docs[i][0]) for i in range(len(training_docs))]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3nSpAvWa_QTD",
        "outputId": "65d3f75e-e1a2-4e2d-80f2-a4e3cbaa285d"
      },
      "source": [
        "neg[2]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['it',\n",
              " 'is',\n",
              " 'not',\n",
              " 'a_NEG',\n",
              " 'mass-market_NEG',\n",
              " 'entertainment_NEG',\n",
              " 'but_NEG',\n",
              " 'an_NEG',\n",
              " 'uncompromising_NEG',\n",
              " 'attempt_NEG',\n",
              " 'by_NEG',\n",
              " 'one_NEG',\n",
              " 'artist_NEG',\n",
              " 'to_NEG',\n",
              " 'think_NEG',\n",
              " 'about_NEG',\n",
              " 'another_NEG',\n",
              " '.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vrzlMILAmY-"
      },
      "source": [
        "* ## 4"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ew-yqXNFdCop"
      },
      "source": [
        "---\n",
        "\n",
        "Define an unigram extractor, named uni_feat_extractor, using the unigram_word_feats function of SentimentAnalyzer. Keep only un-igrams that have a minimum frequency equal to 5 (min_freq = 5)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VGGsMzPo_fJx"
      },
      "source": [
        "from nltk.sentiment import SentimentAnalyzer\n",
        "sentim_analyzer = SentimentAnalyzer()\n",
        "all_words_neg = sentim_analyzer.all_words([mark_negation(doc) for doc in training_docs])\n",
        "uni_feat_extractor = sentim_analyzer.unigram_word_feats(all_words_neg, min_freq=5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "htlCLMuYBAUt",
        "outputId": "ac455f36-ba31-4523-91cc-0bce942247ed"
      },
      "source": [
        "print(uni_feat_extractor)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['.', 'the', ',', 'a', 'and', 'of', 'to', 'is', 'in', 'that', 'his', 'for', 'with', 'it', 'as', 'an', 'on', 'who', 'he', 'by', 'this', 'film', 'has', 'movie', 'from', 'but', 'her', 'are', 'their', 'the_NEG', '\"', 'its', 'one', 'at', '(', 'be', ',_NEG', ')', 'when', 'they', 'a_NEG', 'about', 'not', 'to_NEG', 'into', 'all', 'out', 'so', 'will', 'him', 'life', 'have', 'most', 'of_NEG', 'she', 'story', 'you', 'even', 'like', ';', 'more', \"it's\", 'if', ':', 'what', 'only', \"there's\", 'can', 'love', 'where', 'and_NEG', 'which', 'up', 'take', 'home', 'while', 'both', 'make', 'some', 'work', 'in_NEG', 'get', 'through', 'gets', 'i', '--', 'two', 'or', 'old', 'come', 'beautiful', 'kids', 'just', 'but_NEG', 'much', 'look', 'before', 'it_NEG', 'great', 'very', 'really', 'own', \"doesn't\", 'no', 'young', 'funny', 'far', 'world', 'than', 'makes', 'years', 'way', 'may', 'them', 'new', 'between', 'search', 'family', 'daughter', 'been', 'time', 'never', 'is_NEG', 'after', 'feel', 'begins', 'how', 'people', 'other', 'then', 'best', 'american', 'made', '?', 'set', 'than_NEG', 'being', 'comes', 'boy', 'finds', 'find', 'soon', 'friends']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eKHmvzQEBdZ3"
      },
      "source": [
        "* ## 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x4prkQZ_dF8v"
      },
      "source": [
        "---\n",
        "\n",
        "Add the extractor to the previously defined SentimentAnalyzer () object, using the add_feat_extractor function.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WjuT54tTBCbV"
      },
      "source": [
        "sentim_analyzer.add_feat_extractor(extract_unigram_feats, unigrams=uni_feat_extractor)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KzlcqmauBmxq"
      },
      "source": [
        "* ## 6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vryaTIUOdW6l"
      },
      "source": [
        "---\n",
        "\n",
        "Transform training and testing sets using apply_features function\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChSMZ9X0Bi9A"
      },
      "source": [
        "training_set = sentim_analyzer.apply_features(training_docs)\n",
        "test_set = sentim_analyzer.apply_features(testing_docs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Amt08D-LEVZR"
      },
      "source": [
        "* ## 7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IEzbg6zfdhyh"
      },
      "source": [
        "---\n",
        "\n",
        "Build two classifiers: Naive Bayes and SVM.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRTQdxGOCSfV"
      },
      "source": [
        "from nltk.classify import NaiveBayesClassifier\n",
        "import nltk.classify\n",
        "from sklearn.svm import LinearSVC"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gixIMUG_GUuU",
        "outputId": "03e30a07-a910-4c45-e9da-522a64a87c91"
      },
      "source": [
        "#NaiveBayesClassifier\n",
        "trainer = NaiveBayesClassifier.train\n",
        "NVclassifier = sentim_analyzer.train(trainer, training_set)\n",
        "#SVM \n",
        "SVMclassifier = nltk.classify.SklearnClassifier(LinearSVC())\n",
        "SVMclassifier.train(training_set)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training classifier\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<SklearnClassifier(LinearSVC(C=1.0, class_weight=None, dual=True, fit_intercept=True,\n",
              "          intercept_scaling=1, loss='squared_hinge', max_iter=1000,\n",
              "          multi_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n",
              "          verbose=0))>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RoXtwZ_FH8_w"
      },
      "source": [
        "* ## 8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldatWiJZd0z2"
      },
      "source": [
        "----\n",
        "\n",
        "Test the classifiers obtained on the test set and display the evaluation metrics\n",
        "using sentim_analyzer.evaluate (test_set)\n",
        "\n",
        "-----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZI4wncCsP6tt"
      },
      "source": [
        "For naive bayses classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UTx-9CGtHDzF",
        "outputId": "3c35c9bc-5c5c-45ad-dc92-711e2517b2aa"
      },
      "source": [
        "for key,value in sorted(sentim_analyzer.evaluate(test_set).items()):\n",
        "   print('{0}: {1}'.format(key, value))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluating NaiveBayesClassifier results...\n",
            "Accuracy: 0.75\n",
            "F-measure [obj]: 0.7435897435897436\n",
            "F-measure [subj]: 0.7560975609756098\n",
            "Precision [obj]: 0.7631578947368421\n",
            "Precision [subj]: 0.7380952380952381\n",
            "Recall [obj]: 0.725\n",
            "Recall [subj]: 0.775\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ddnG8dOgQC9Y"
      },
      "source": [
        "For SVM"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VEfpZEyLLhk7",
        "outputId": "f034876e-db09-43d5-a18d-af91b8b55db8"
      },
      "source": [
        "# get the accuracy using svm\n",
        "accuracy = nltk.classify.accuracy(SVMclassifier, test_set)\n",
        "print(accuracy)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.7375\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "frZHB6rkd5JB"
      },
      "source": [
        "----\n",
        "----\n",
        "The best model is NaiveBayesClassifier ( it has the best accuracy )\n",
        "\n",
        "----\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YKXrICPhUIHX"
      },
      "source": [
        "* ## 9"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnJhkBr8KeKB"
      },
      "source": [
        "----\n",
        "\n",
        "Using the best model, we gonna predict the subjectivity of each document for the Brown corpus. Don't forget to transform the data using apply_features. And let's store the result in a Test_on_brown dictionary whose key is the document identifier (first document ID = 1) and the value indicates the subjectivity of the document (\"obj\" or \"subj\").\n",
        "\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EuY_oI6gUTKi",
        "outputId": "d27a6eab-7e48-4c1f-f710-db1597bfd13a"
      },
      "source": [
        "from nltk.corpus import brown\n",
        "import nltk\n",
        "nltk.download('brown')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/brown.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VbXRudj_K7xp"
      },
      "source": [
        "Preparing our corpus data "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y6csBlHYD9y1"
      },
      "source": [
        "brown_data = []\n",
        " \n",
        "for fileid in brown.fileids():\n",
        "    document = ' '.join(brown.words(fileid))\n",
        "    brown_data.append(document)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9rSntL0LA_3"
      },
      "source": [
        "----"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QmtXPV20FHBX"
      },
      "source": [
        "brown_set = sentim_analyzer.apply_features(brown_data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wbl_LvZHDyAv"
      },
      "source": [
        "Test_on_brown={i+1:NVclassifier.classify(brown_set[i]) for i in range(0,len(brown_set))}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JKpGRf6OIRCT"
      },
      "source": [
        "* ## 10\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f3nmPLGJLHbN"
      },
      "source": [
        "----\n",
        "Let's the predicted subjectivity for document number 403 \n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "PA459zgbOlZi",
        "outputId": "694ca328-dbf4-4c12-8432-8aa04e9ab11d"
      },
      "source": [
        "Test_on_brown[403]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'obj'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6_vzgk2Vf3NR"
      },
      "source": [
        "# Lexicon based Sentiment Analysis - Application à la langue Arabe"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0txvpF4znII"
      },
      "source": [
        "----\n",
        "\n",
        "Develop a Senti_Analyzer_Ar function which takes as input a lexicon and an item in Arabic and returns the polarity of the item. The Lexicon to use is: Arabic Hashtag Lexicon (dialect), download from http://saifmohammad.com/WebPages/ArabicSA.html.\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GgDQ1p9G1gpC"
      },
      "source": [
        "----\n",
        "\n",
        "Downloading the arabic lexicon\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v1DgbxA4ci5O",
        "outputId": "5f2232e6-3d52-4ebc-bc7f-1e29039e4ded"
      },
      "source": [
        "!wget http://saifmohammad.com/WebDocs/Arabic%20Lexicons/Arabic_Hashtag_Lexicon_dialectal.txt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-01-12 21:15:09--  http://saifmohammad.com/WebDocs/Arabic%20Lexicons/Arabic_Hashtag_Lexicon_dialectal.txt\n",
            "Resolving saifmohammad.com (saifmohammad.com)... 192.185.17.122\n",
            "Connecting to saifmohammad.com (saifmohammad.com)|192.185.17.122|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 616725 (602K) [text/plain]\n",
            "Saving to: ‘Arabic_Hashtag_Lexicon_dialectal.txt’\n",
            "\n",
            "Arabic_Hashtag_Lexi 100%[===================>] 602.27K  2.05MB/s    in 0.3s    \n",
            "\n",
            "2021-01-12 21:15:10 (2.05 MB/s) - ‘Arabic_Hashtag_Lexicon_dialectal.txt’ saved [616725/616725]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wloUfRcd2MRc"
      },
      "source": [
        "----\n",
        "\n",
        "We have to delete the information in the top of the file before uploading it "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dLIbFalEdYte"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        " \n",
        "lexicon = pd.read_csv('/content/Arabic_Hashtag_Lexicon_dialectal.txt', delimiter=\"\\t\", header=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "id": "2VMXTkM0eC3l",
        "outputId": "2c847400-e495-41c5-dcde-5f5fc389a3c6"
      },
      "source": [
        "lexicon.tail()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>[Arabic Term]</th>\n",
              "      <th>[Buckwalter]</th>\n",
              "      <th>[Sentiment Score]</th>\n",
              "      <th>[Positive Occurrence Count]</th>\n",
              "      <th>[Negative Occurrence Count]</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>20122</th>\n",
              "      <td>العولقي</td>\n",
              "      <td>AlEwlqy</td>\n",
              "      <td>1.219</td>\n",
              "      <td>12</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20123</th>\n",
              "      <td>مغلق</td>\n",
              "      <td>mglq</td>\n",
              "      <td>0.446</td>\n",
              "      <td>23</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20124</th>\n",
              "      <td>جبلي</td>\n",
              "      <td>jbly</td>\n",
              "      <td>1.487</td>\n",
              "      <td>16</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20125</th>\n",
              "      <td>نورة_الخطيب</td>\n",
              "      <td>nwrp_AlxTyb</td>\n",
              "      <td>0.600</td>\n",
              "      <td>6</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20126</th>\n",
              "      <td>يوتيوب</td>\n",
              "      <td>ywtywb</td>\n",
              "      <td>1.793</td>\n",
              "      <td>322</td>\n",
              "      <td>13.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      [Arabic Term]  ... [Negative Occurrence Count]\n",
              "20122       العولقي  ...                         0.0\n",
              "20123          مغلق  ...                         3.0\n",
              "20124          جبلي  ...                         0.0\n",
              "20125   نورة_الخطيب  ...                         0.0\n",
              "20126        يوتيوب  ...                        13.0\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EL2WMwWO22a6"
      },
      "source": [
        "----\n",
        "----"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hpuBi_6eGvL"
      },
      "source": [
        "def Senti_Analyzer_Ar(lexicon, item):\n",
        "\n",
        "  for i in range(len(lexicon['[Arabic Term]'])):\n",
        "    if lexicon['[Arabic Term]'][i]== item:\n",
        "      pola = lexicon['[Sentiment Score]'][i]\n",
        "      break\n",
        "    else: \n",
        "      pola = 0\n",
        "  return pola"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jQl4mu-CnDMY"
      },
      "source": [
        "---\n",
        "\n",
        "Test for the word 'اط'\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_u5O_Qjem68I",
        "outputId": "d2bf75ed-4308-417b-e990-f07098f2387f"
      },
      "source": [
        "pola = Senti_Analyzer_Ar(lexicon, 'اط')\n",
        "if pola  > -1:\n",
        "            print('positive')\n",
        "elif pola == 0:\n",
        "            print('neutral')\n",
        "elif pola  <= -1:\n",
        "            print('negative')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "positive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TkUDmXTM4Ca8"
      },
      "source": [
        "----\n",
        "\n",
        "Let‘s use the Senti_Analyzer_Ar function to find the polarity of the items contained in the ”comments.csv” file\n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GFv-cGfopTKZ"
      },
      "source": [
        "comments = pd.read_excel('/content/comments.xlsx')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "qcyYfSiWq6ME",
        "outputId": "8c495087-a3b4-4c72-8e2c-f949223b2fae"
      },
      "source": [
        "comments.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>Unnamed: 1</th>\n",
              "      <th>Unnamed: 2</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>ههههههههه ما حدا احسن من حدا .. عشان تحسو فينا...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>تحزنوا هاهاهاهاها</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>اللي بحكي انو الفيصلي عنده عقده اسمها شباب الأ...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>يعني ادارة الفيصلي لما شافت فريقك الاول (الماج...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>اوساسونا ريال سرقسطة</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text Unnamed: 1 Unnamed: 2\n",
              "0  ههههههههه ما حدا احسن من حدا .. عشان تحسو فينا...        NaN        NaN\n",
              "1                                  تحزنوا هاهاهاهاها        NaN        NaN\n",
              "2  اللي بحكي انو الفيصلي عنده عقده اسمها شباب الأ...        NaN        NaN\n",
              "3  يعني ادارة الفيصلي لما شافت فريقك الاول (الماج...        NaN        NaN\n",
              "4                               اوساسونا ريال سرقسطة        NaN        NaN"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQrbGHwG4jEk"
      },
      "source": [
        "---\n",
        "\n",
        "Putting the results in a dict \n",
        "\n",
        "----"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPPff0b4sMRw"
      },
      "source": [
        "polarity_dict = {}\n",
        "for i in range(len(comments['text'])):\n",
        "  pola = 0\n",
        "  for item in comments['text'][i].split():\n",
        "    pola_i = Senti_Analyzer_Ar(lexicon, item)\n",
        "    pola+=pola_i\n",
        "    tmp = pola\n",
        "  polarity_dict.__setitem__('comment %d polarity '% i , tmp)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWfXUWJI6z49"
      },
      "source": [
        "----\n",
        "\n",
        "Comment 2  polarity "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOhXfUWyz51Q",
        "outputId": "7c39622b-919b-4d99-838e-53493db89313"
      },
      "source": [
        "polarity_dict['comment 2 polarity ']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.306"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    }
  ]
}